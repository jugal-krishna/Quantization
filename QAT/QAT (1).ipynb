{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f3174f697ce4ffdaa400b5397175caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e25c33aebdd54a32ab372276da851a65",
              "IPY_MODEL_17e90a5325d94f4881066712971e77bd",
              "IPY_MODEL_8cad15043d024aa8930530311b50d54b"
            ],
            "layout": "IPY_MODEL_227676e5ba1542d9848216dbecdfc1d7"
          }
        },
        "e25c33aebdd54a32ab372276da851a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca7d112caef4dbe87dedf7627847e08",
            "placeholder": "​",
            "style": "IPY_MODEL_e88bdbda8cce46f985fd7a2a28fa99db",
            "value": ""
          }
        },
        "17e90a5325d94f4881066712971e77bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c5bd5c9be36428bb379e76d2a4e7041",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9726b7986894d5ab030657741cc72c8",
            "value": 170498071
          }
        },
        "8cad15043d024aa8930530311b50d54b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1a2c4dc3a341c184512e75ad4043f9",
            "placeholder": "​",
            "style": "IPY_MODEL_f17a0fbc1d1c406da612c6364f375e9b",
            "value": " 170499072/? [00:03&lt;00:00, 52297403.27it/s]"
          }
        },
        "227676e5ba1542d9848216dbecdfc1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca7d112caef4dbe87dedf7627847e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88bdbda8cce46f985fd7a2a28fa99db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c5bd5c9be36428bb379e76d2a4e7041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9726b7986894d5ab030657741cc72c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f1a2c4dc3a341c184512e75ad4043f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17a0fbc1d1c406da612c6364f375e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "sK8v8y9xwPr7",
        "outputId": "06cdb45e-517c-413c-a552-7af878a5797d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef43f08e-b4cb-4bdc-9b50-4508c84ba344\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef43f08e-b4cb-4bdc-9b50-4508c84ba344\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resnet.py to resnet.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resnet.py': b'# -*- coding: utf-8 -*-\\n\"\"\"resnet.py\\n\\nAutomatically generated by Colaboratory.\\n\\nOriginal file is located at\\n    https://colab.research.google.com/drive/1KdUW3evXFmHZptv8q0fl0lhX1Gr-igcw\\n\"\"\"\\n\\n# Modified from\\n# https://github.com/pytorch/vision/blob/release/0.8.0/torchvision/models/resnet.py\\n\\nimport torch\\nfrom torch import Tensor\\nimport torch.nn as nn\\nfrom torch.hub import load_state_dict_from_url\\nfrom typing import Type, Any, Callable, Union, List, Optional\\n\\n\\n__all__ = [\\'ResNet\\', \\'resnet18\\', \\'resnet34\\', \\'resnet50\\', \\'resnet101\\',\\n           \\'resnet152\\', \\'resnext50_32x4d\\', \\'resnext101_32x8d\\',\\n           \\'wide_resnet50_2\\', \\'wide_resnet101_2\\']\\n\\n\\nmodel_urls = {\\n    \\'resnet18\\': \\'https://download.pytorch.org/models/resnet18-5c106cde.pth\\',\\n    \\'resnet34\\': \\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\\',\\n    \\'resnet50\\': \\'https://download.pytorch.org/models/resnet50-19c8e357.pth\\',\\n    \\'resnet101\\': \\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\\',\\n    \\'resnet152\\': \\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\\',\\n    \\'resnext50_32x4d\\': \\'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\\',\\n    \\'resnext101_32x8d\\': \\'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\\',\\n    \\'wide_resnet50_2\\': \\'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\\',\\n    \\'wide_resnet101_2\\': \\'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\\',\\n}\\n\\n\\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\\n    \"\"\"3x3 convolution with padding\"\"\"\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\\n\\n\\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\\n    \"\"\"1x1 convolution\"\"\"\\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\\n\\nclass BasicBlock(nn.Module):\\n    expansion: int = 1\\n\\n    def __init__(\\n        self,\\n        inplanes: int,\\n        planes: int,\\n        stride: int = 1,\\n        downsample: Optional[nn.Module] = None,\\n        groups: int = 1,\\n        base_width: int = 64,\\n        dilation: int = 1,\\n        norm_layer: Optional[Callable[..., nn.Module]] = None\\n    ) -> None:\\n        super(BasicBlock, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        if groups != 1 or base_width != 64:\\n            raise ValueError(\\'BasicBlock only supports groups=1 and base_width=64\\')\\n        if dilation > 1:\\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\\n        self.conv1 = conv3x3(inplanes, planes, stride)\\n        self.bn1 = norm_layer(planes)\\n        # Rename relu to relu1\\n        self.relu1 = nn.ReLU(inplace=True)\\n        self.conv2 = conv3x3(planes, planes)\\n        self.bn2 = norm_layer(planes)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.skip_add = nn.quantized.FloatFunctional()\\n        # Remember to use two independent ReLU for layer fusion.\\n        self.relu2 = nn.ReLU(inplace=True)\\n\\n    def forward(self, x: Tensor) -> Tensor:\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu1(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n        \\n        # Use FloatFunctional for addition for quantization compatibility\\n        # out += identity\\n        out = self.skip_add.add(identity, out)\\n        out = self.relu2(out)\\n\\n        return out\\n\\n\\nclass Bottleneck(nn.Module):\\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\\n\\n    expansion: int = 4\\n\\n    def __init__(\\n        self,\\n        inplanes: int,\\n        planes: int,\\n        stride: int = 1,\\n        downsample: Optional[nn.Module] = None,\\n        groups: int = 1,\\n        base_width: int = 64,\\n        dilation: int = 1,\\n        norm_layer: Optional[Callable[..., nn.Module]] = None\\n    ) -> None:\\n        super(Bottleneck, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        width = int(planes * (base_width / 64.)) * groups\\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\\n        self.conv1 = conv1x1(inplanes, width)\\n        self.bn1 = norm_layer(width)\\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\\n        self.bn2 = norm_layer(width)\\n        self.conv3 = conv1x1(width, planes * self.expansion)\\n        self.bn3 = norm_layer(planes * self.expansion)\\n        self.relu1 = nn.ReLU(inplace=True)\\n        self.downsample = downsample\\n        self.stride = stride\\n        self.skip_add = nn.quantized.FloatFunctional()\\n        self.relu2 = nn.ReLU(inplace=True)\\n\\n    def forward(self, x: Tensor) -> Tensor:\\n        identity = x\\n\\n        out = self.conv1(x)\\n        out = self.bn1(out)\\n        out = self.relu1(out)\\n\\n        out = self.conv2(out)\\n        out = self.bn2(out)\\n        out = self.relu(out)\\n\\n        out = self.conv3(out)\\n        out = self.bn3(out)\\n\\n        if self.downsample is not None:\\n            identity = self.downsample(x)\\n\\n        # out += identity\\n        out = self.skip_add.add(identity, out)\\n        out = self.relu2(out)\\n\\n        return out\\n\\n\\nclass ResNet(nn.Module):\\n\\n    def __init__(\\n        self,\\n        block: Type[Union[BasicBlock, Bottleneck]],\\n        layers: List[int],\\n        num_classes: int = 1000,\\n        zero_init_residual: bool = False,\\n        groups: int = 1,\\n        width_per_group: int = 64,\\n        replace_stride_with_dilation: Optional[List[bool]] = None,\\n        norm_layer: Optional[Callable[..., nn.Module]] = None\\n    ) -> None:\\n        super(ResNet, self).__init__()\\n        if norm_layer is None:\\n            norm_layer = nn.BatchNorm2d\\n        self._norm_layer = norm_layer\\n\\n        self.inplanes = 64\\n        self.dilation = 1\\n        if replace_stride_with_dilation is None:\\n            # each element in the tuple indicates if we should replace\\n            # the 2x2 stride with a dilated convolution instead\\n            replace_stride_with_dilation = [False, False, False]\\n        if len(replace_stride_with_dilation) != 3:\\n            raise ValueError(\"replace_stride_with_dilation should be None \"\\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\\n        self.groups = groups\\n        self.base_width = width_per_group\\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\\n                               bias=False)\\n        self.bn1 = norm_layer(self.inplanes)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\\n        self.layer1 = self._make_layer(block, 64, layers[0])\\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\\n                                       dilate=replace_stride_with_dilation[0])\\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\\n                                       dilate=replace_stride_with_dilation[1])\\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\\n                                       dilate=replace_stride_with_dilation[2])\\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\\n\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv2d):\\n                nn.init.kaiming_normal_(m.weight, mode=\\'fan_out\\', nonlinearity=\\'relu\\')\\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\\n                nn.init.constant_(m.weight, 1)\\n                nn.init.constant_(m.bias, 0)\\n\\n        # Zero-initialize the last BN in each residual branch,\\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\\n        if zero_init_residual:\\n            for m in self.modules():\\n                if isinstance(m, Bottleneck):\\n                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\\n                elif isinstance(m, BasicBlock):\\n                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\\n\\n    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\\n                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\\n        norm_layer = self._norm_layer\\n        downsample = None\\n        previous_dilation = self.dilation\\n        if dilate:\\n            self.dilation *= stride\\n            stride = 1\\n        if stride != 1 or self.inplanes != planes * block.expansion:\\n            downsample = nn.Sequential(\\n                conv1x1(self.inplanes, planes * block.expansion, stride),\\n                norm_layer(planes * block.expansion),\\n            )\\n\\n        layers = []\\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\\n                            self.base_width, previous_dilation, norm_layer))\\n        self.inplanes = planes * block.expansion\\n        for _ in range(1, blocks):\\n            layers.append(block(self.inplanes, planes, groups=self.groups,\\n                                base_width=self.base_width, dilation=self.dilation,\\n                                norm_layer=norm_layer))\\n\\n        return nn.Sequential(*layers)\\n\\n    def _forward_impl(self, x: Tensor) -> Tensor:\\n        # See note [TorchScript super()]\\n        x = self.conv1(x)\\n        x = self.bn1(x)\\n        x = self.relu(x)\\n        x = self.maxpool(x)\\n\\n        x = self.layer1(x)\\n        x = self.layer2(x)\\n        x = self.layer3(x)\\n        x = self.layer4(x)\\n\\n        x = self.avgpool(x)\\n        x = torch.flatten(x, 1)\\n        x = self.fc(x)\\n\\n        return x\\n\\n    def forward(self, x: Tensor) -> Tensor:\\n        return self._forward_impl(x)\\n\\n\\ndef _resnet(\\n    arch: str,\\n    block: Type[Union[BasicBlock, Bottleneck]],\\n    layers: List[int],\\n    pretrained: bool,\\n    progress: bool,\\n    **kwargs: Any\\n) -> ResNet:\\n    model = ResNet(block, layers, **kwargs)\\n    if pretrained:\\n        state_dict = load_state_dict_from_url(model_urls[arch],\\n                                              progress=progress)\\n        model.load_state_dict(state_dict)\\n    return model\\n\\n\\ndef resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNet-18 model from\\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    return _resnet(\\'resnet18\\', BasicBlock, [2, 2, 2, 2], pretrained, progress,\\n                   **kwargs)\\n\\n\\ndef resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNet-34 model from\\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    return _resnet(\\'resnet34\\', BasicBlock, [3, 4, 6, 3], pretrained, progress,\\n                   **kwargs)\\n\\n\\ndef resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNet-50 model from\\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    return _resnet(\\'resnet50\\', Bottleneck, [3, 4, 6, 3], pretrained, progress,\\n                   **kwargs)\\n\\n\\ndef resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNet-101 model from\\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    return _resnet(\\'resnet101\\', Bottleneck, [3, 4, 23, 3], pretrained, progress,\\n                   **kwargs)\\n\\n\\ndef resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNet-152 model from\\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    return _resnet(\\'resnet152\\', Bottleneck, [3, 8, 36, 3], pretrained, progress,\\n                   **kwargs)\\n\\n\\ndef resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNeXt-50 32x4d model from\\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 4\\n    return _resnet(\\'resnext50_32x4d\\', Bottleneck, [3, 4, 6, 3],\\n                   pretrained, progress, **kwargs)\\n\\n\\ndef resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"ResNeXt-101 32x8d model from\\n    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    kwargs[\\'groups\\'] = 32\\n    kwargs[\\'width_per_group\\'] = 8\\n    return _resnet(\\'resnext101_32x8d\\', Bottleneck, [3, 4, 23, 3],\\n                   pretrained, progress, **kwargs)\\n\\n\\ndef wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"Wide ResNet-50-2 model from\\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\\n\\n    The model is the same as ResNet except for the bottleneck number of channels\\n    which is twice larger in every block. The number of channels in outer 1x1\\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _resnet(\\'wide_resnet50_2\\', Bottleneck, [3, 4, 6, 3],\\n                   pretrained, progress, **kwargs)\\n\\n\\ndef wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\\n    r\"\"\"Wide ResNet-101-2 model from\\n    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\\n\\n    The model is the same as ResNet except for the bottleneck number of channels\\n    which is twice larger in every block. The number of channels in outer 1x1\\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\\n\\n    Args:\\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\\n        progress (bool): If True, displays a progress bar of the download to stderr\\n    \"\"\"\\n    kwargs[\\'width_per_group\\'] = 64 * 2\\n    return _resnet(\\'wide_resnet101_2\\', Bottleneck, [3, 4, 23, 3],\\n                   pretrained, progress, **kwargs)\\n\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from resnet import resnet18"
      ],
      "metadata": {
        "id": "8hXrPBLYwhUK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ],
      "metadata": {
        "id": "z47anIEfy4qG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
        "    # We will use test set for validation and test in this project.\n",
        "    # Do not use test set for validation in practice!\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "o-qANuoizpXe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=20):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rmElTWvHz_iu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "Ss7VE4e_ztZ-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)"
      ],
      "metadata": {
        "id": "zE90jLxz0Eyd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "XK-eAkq50Kkr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n"
      ],
      "metadata": {
        "id": "plX48HtM0Plb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Trd3EQfT0Sr1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "\n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hAlqDSCW0VM0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "\n",
        "    # The number of channels in ResNet18 is divisible by 8.\n",
        "    # This is required for fast GEMM integer matrix multiplication.\n",
        "    # model = torchvision.models.resnet18(pretrained=False)\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "\n",
        "    # We would use the pretrained ResNet18 as a feature extractor.\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    \n",
        "    # Modify the last FC layer\n",
        "    # num_features = model.fc.in_features\n",
        "    # model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_xFHNVjI0YnO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "K4bhShxg0wMW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "1x96aROe001d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    random_seed = 0\n",
        "    num_classes = 10\n",
        "    cuda_device = torch.device(\"cuda:0\")\n",
        "    cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "    model_dir = \"saved_models\"\n",
        "    model_filename = \"resnet18_cifar10.pt\"\n",
        "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "    set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "    # Create an untrained model.\n",
        "    model = create_model(num_classes=num_classes)\n",
        "\n",
        "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "    \n",
        "    # Train model.\n",
        "    print(\"Training Model...\")\n",
        "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)\n",
        "    # Save model.\n",
        "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "    # Load a pretrained model.\n",
        "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
        "    model.to(cpu_device)\n",
        "    # Make a copy of the model for layer fusion\n",
        "    fused_model = copy.deepcopy(model)\n",
        "\n",
        "    model.eval()\n",
        "    # The model has to be switched to training mode before any layer fusion.\n",
        "    # Otherwise the quantization aware training will not work correctly.\n",
        "    fused_model.eval()\n",
        "\n",
        "    # Fuse the model in place rather manually.\n",
        "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "    for module_name, module in fused_model.named_children():\n",
        "        if \"layer\" in module_name:\n",
        "            for basic_block_name, basic_block in module.named_children():\n",
        "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "                for sub_block_name, sub_block in basic_block.named_children():\n",
        "                    if sub_block_name == \"downsample\":\n",
        "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "    # Print FP32 model.\n",
        "    print(model)\n",
        "    # Print fused model.\n",
        "    print(fused_model)\n",
        "\n",
        "    # Model and fused model should be equivalent.\n",
        "    model.eval()\n",
        "    fused_model.eval()\n",
        "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
        "\n",
        "    # Prepare the model for quantization aware training. This inserts observers in\n",
        "    # the model that will observe activation tensors during calibration.\n",
        "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "    # Using un-fused model will fail.\n",
        "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "    # Select quantization schemes from \n",
        "    # https://pytorch.org/docs/stable/quantization-support.html\n",
        "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "    # Custom quantization configurations\n",
        "    # quantization_config = torch.quantization.default_qconfig\n",
        "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "    quantized_model.qconfig = quantization_config\n",
        "    \n",
        "    # Print quantization configurations\n",
        "    print(quantized_model.qconfig)\n",
        "\n",
        "    # https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "    torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "    # # Use training data for calibration.\n",
        "    print(\"Training QAT Model...\")\n",
        "    quantized_model.train()\n",
        "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
        "    quantized_model.to(cpu_device)\n",
        "\n",
        "    # Using high-level static quantization wrapper\n",
        "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "    quantized_model.eval()\n",
        "\n",
        "    # Print quantized model.\n",
        "    print(quantized_model)\n",
        "\n",
        "    # Save quantized model.\n",
        "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "    # Load quantized model.\n",
        "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "    _, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "    # Skip this assertion since the values might deviate a lot.\n",
        "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    \n",
        "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n"
      ],
      "metadata": {
        "id": "ezvvmiEK040i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "GahJa6-d1QV1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6f3174f697ce4ffdaa400b5397175caf",
            "e25c33aebdd54a32ab372276da851a65",
            "17e90a5325d94f4881066712971e77bd",
            "8cad15043d024aa8930530311b50d54b",
            "227676e5ba1542d9848216dbecdfc1d7",
            "0ca7d112caef4dbe87dedf7627847e08",
            "e88bdbda8cce46f985fd7a2a28fa99db",
            "7c5bd5c9be36428bb379e76d2a4e7041",
            "d9726b7986894d5ab030657741cc72c8",
            "8f1a2c4dc3a341c184512e75ad4043f9",
            "f17a0fbc1d1c406da612c6364f375e9b"
          ]
        },
        "outputId": "d0cae2b6-adc5-4e28-a1f7-c565083eb697"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f3174f697ce4ffdaa400b5397175caf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n",
            "Epoch: -1 Eval Loss: 2.325 Eval Acc: 0.098\n",
            "Epoch: 000 Train Loss: 2.128 Train Acc: 0.286 Eval Loss: 1.742 Eval Acc: 0.384\n",
            "Epoch: 001 Train Loss: 1.540 Train Acc: 0.430 Eval Loss: 1.470 Eval Acc: 0.462\n",
            "Epoch: 002 Train Loss: 1.364 Train Acc: 0.502 Eval Loss: 1.222 Eval Acc: 0.559\n",
            "Epoch: 003 Train Loss: 1.217 Train Acc: 0.562 Eval Loss: 1.162 Eval Acc: 0.585\n",
            "Epoch: 004 Train Loss: 1.096 Train Acc: 0.610 Eval Loss: 1.022 Eval Acc: 0.645\n",
            "Epoch: 005 Train Loss: 1.005 Train Acc: 0.644 Eval Loss: 1.013 Eval Acc: 0.644\n",
            "Epoch: 006 Train Loss: 0.938 Train Acc: 0.669 Eval Loss: 0.908 Eval Acc: 0.686\n",
            "Epoch: 007 Train Loss: 0.893 Train Acc: 0.685 Eval Loss: 0.813 Eval Acc: 0.717\n",
            "Epoch: 008 Train Loss: 0.832 Train Acc: 0.709 Eval Loss: 0.797 Eval Acc: 0.716\n",
            "Epoch: 009 Train Loss: 0.802 Train Acc: 0.721 Eval Loss: 0.811 Eval Acc: 0.723\n",
            "Epoch: 010 Train Loss: 0.762 Train Acc: 0.735 Eval Loss: 0.794 Eval Acc: 0.725\n",
            "Epoch: 011 Train Loss: 0.737 Train Acc: 0.743 Eval Loss: 0.742 Eval Acc: 0.744\n",
            "Epoch: 012 Train Loss: 0.714 Train Acc: 0.754 Eval Loss: 0.729 Eval Acc: 0.748\n",
            "Epoch: 013 Train Loss: 0.687 Train Acc: 0.761 Eval Loss: 0.722 Eval Acc: 0.753\n",
            "Epoch: 014 Train Loss: 0.676 Train Acc: 0.765 Eval Loss: 0.683 Eval Acc: 0.767\n",
            "Epoch: 015 Train Loss: 0.655 Train Acc: 0.772 Eval Loss: 0.695 Eval Acc: 0.758\n",
            "Epoch: 016 Train Loss: 0.637 Train Acc: 0.778 Eval Loss: 0.678 Eval Acc: 0.769\n",
            "Epoch: 017 Train Loss: 0.616 Train Acc: 0.786 Eval Loss: 0.663 Eval Acc: 0.775\n",
            "Epoch: 018 Train Loss: 0.612 Train Acc: 0.788 Eval Loss: 0.683 Eval Acc: 0.772\n",
            "Epoch: 019 Train Loss: 0.600 Train Acc: 0.793 Eval Loss: 0.646 Eval Acc: 0.779\n",
            "Epoch: 020 Train Loss: 0.587 Train Acc: 0.795 Eval Loss: 0.669 Eval Acc: 0.778\n",
            "Epoch: 021 Train Loss: 0.576 Train Acc: 0.799 Eval Loss: 0.663 Eval Acc: 0.780\n",
            "Epoch: 022 Train Loss: 0.565 Train Acc: 0.803 Eval Loss: 0.739 Eval Acc: 0.752\n",
            "Epoch: 023 Train Loss: 0.558 Train Acc: 0.806 Eval Loss: 0.666 Eval Acc: 0.775\n",
            "Epoch: 024 Train Loss: 0.548 Train Acc: 0.809 Eval Loss: 0.640 Eval Acc: 0.787\n",
            "Epoch: 025 Train Loss: 0.539 Train Acc: 0.813 Eval Loss: 0.647 Eval Acc: 0.781\n",
            "Epoch: 026 Train Loss: 0.534 Train Acc: 0.812 Eval Loss: 0.618 Eval Acc: 0.788\n",
            "Epoch: 027 Train Loss: 0.526 Train Acc: 0.817 Eval Loss: 0.591 Eval Acc: 0.794\n",
            "Epoch: 028 Train Loss: 0.517 Train Acc: 0.820 Eval Loss: 0.600 Eval Acc: 0.797\n",
            "Epoch: 029 Train Loss: 0.509 Train Acc: 0.822 Eval Loss: 0.648 Eval Acc: 0.784\n",
            "Epoch: 030 Train Loss: 0.505 Train Acc: 0.824 Eval Loss: 0.617 Eval Acc: 0.789\n",
            "Epoch: 031 Train Loss: 0.495 Train Acc: 0.827 Eval Loss: 0.607 Eval Acc: 0.801\n",
            "Epoch: 032 Train Loss: 0.492 Train Acc: 0.829 Eval Loss: 0.566 Eval Acc: 0.811\n",
            "Epoch: 033 Train Loss: 0.485 Train Acc: 0.831 Eval Loss: 0.611 Eval Acc: 0.793\n",
            "Epoch: 034 Train Loss: 0.481 Train Acc: 0.833 Eval Loss: 0.669 Eval Acc: 0.780\n",
            "Epoch: 035 Train Loss: 0.477 Train Acc: 0.833 Eval Loss: 0.652 Eval Acc: 0.787\n",
            "Epoch: 036 Train Loss: 0.469 Train Acc: 0.834 Eval Loss: 0.610 Eval Acc: 0.799\n",
            "Epoch: 037 Train Loss: 0.467 Train Acc: 0.837 Eval Loss: 0.612 Eval Acc: 0.793\n",
            "Epoch: 038 Train Loss: 0.460 Train Acc: 0.839 Eval Loss: 0.627 Eval Acc: 0.796\n",
            "Epoch: 039 Train Loss: 0.457 Train Acc: 0.841 Eval Loss: 0.578 Eval Acc: 0.812\n",
            "Epoch: 040 Train Loss: 0.446 Train Acc: 0.846 Eval Loss: 0.613 Eval Acc: 0.801\n",
            "Epoch: 041 Train Loss: 0.447 Train Acc: 0.844 Eval Loss: 0.647 Eval Acc: 0.784\n",
            "Epoch: 042 Train Loss: 0.444 Train Acc: 0.844 Eval Loss: 0.607 Eval Acc: 0.796\n",
            "Epoch: 043 Train Loss: 0.444 Train Acc: 0.845 Eval Loss: 0.560 Eval Acc: 0.810\n",
            "Epoch: 044 Train Loss: 0.439 Train Acc: 0.846 Eval Loss: 0.566 Eval Acc: 0.808\n",
            "Epoch: 045 Train Loss: 0.429 Train Acc: 0.848 Eval Loss: 0.611 Eval Acc: 0.801\n",
            "Epoch: 046 Train Loss: 0.433 Train Acc: 0.848 Eval Loss: 0.653 Eval Acc: 0.787\n",
            "Epoch: 047 Train Loss: 0.425 Train Acc: 0.851 Eval Loss: 0.631 Eval Acc: 0.794\n",
            "Epoch: 048 Train Loss: 0.420 Train Acc: 0.853 Eval Loss: 0.566 Eval Acc: 0.811\n",
            "Epoch: 049 Train Loss: 0.423 Train Acc: 0.852 Eval Loss: 0.570 Eval Acc: 0.812\n",
            "Epoch: 050 Train Loss: 0.418 Train Acc: 0.855 Eval Loss: 0.594 Eval Acc: 0.804\n",
            "Epoch: 051 Train Loss: 0.414 Train Acc: 0.854 Eval Loss: 0.584 Eval Acc: 0.809\n",
            "Epoch: 052 Train Loss: 0.408 Train Acc: 0.857 Eval Loss: 0.589 Eval Acc: 0.812\n",
            "Epoch: 053 Train Loss: 0.406 Train Acc: 0.856 Eval Loss: 0.572 Eval Acc: 0.811\n",
            "Epoch: 054 Train Loss: 0.408 Train Acc: 0.858 Eval Loss: 0.546 Eval Acc: 0.816\n",
            "Epoch: 055 Train Loss: 0.399 Train Acc: 0.859 Eval Loss: 0.587 Eval Acc: 0.818\n",
            "Epoch: 056 Train Loss: 0.402 Train Acc: 0.859 Eval Loss: 0.569 Eval Acc: 0.814\n",
            "Epoch: 057 Train Loss: 0.402 Train Acc: 0.859 Eval Loss: 0.549 Eval Acc: 0.818\n",
            "Epoch: 058 Train Loss: 0.395 Train Acc: 0.861 Eval Loss: 0.581 Eval Acc: 0.815\n",
            "Epoch: 059 Train Loss: 0.400 Train Acc: 0.862 Eval Loss: 0.624 Eval Acc: 0.797\n",
            "Epoch: 060 Train Loss: 0.390 Train Acc: 0.863 Eval Loss: 0.626 Eval Acc: 0.804\n",
            "Epoch: 061 Train Loss: 0.388 Train Acc: 0.866 Eval Loss: 0.562 Eval Acc: 0.818\n",
            "Epoch: 062 Train Loss: 0.390 Train Acc: 0.864 Eval Loss: 0.568 Eval Acc: 0.817\n",
            "Epoch: 063 Train Loss: 0.382 Train Acc: 0.866 Eval Loss: 0.583 Eval Acc: 0.814\n",
            "Epoch: 064 Train Loss: 0.388 Train Acc: 0.863 Eval Loss: 0.558 Eval Acc: 0.822\n",
            "Epoch: 065 Train Loss: 0.386 Train Acc: 0.865 Eval Loss: 0.630 Eval Acc: 0.801\n",
            "Epoch: 066 Train Loss: 0.381 Train Acc: 0.867 Eval Loss: 0.549 Eval Acc: 0.822\n",
            "Epoch: 067 Train Loss: 0.376 Train Acc: 0.869 Eval Loss: 0.594 Eval Acc: 0.823\n",
            "Epoch: 068 Train Loss: 0.380 Train Acc: 0.867 Eval Loss: 0.570 Eval Acc: 0.814\n",
            "Epoch: 069 Train Loss: 0.373 Train Acc: 0.870 Eval Loss: 0.549 Eval Acc: 0.817\n",
            "Epoch: 070 Train Loss: 0.366 Train Acc: 0.872 Eval Loss: 0.619 Eval Acc: 0.807\n",
            "Epoch: 071 Train Loss: 0.369 Train Acc: 0.871 Eval Loss: 0.579 Eval Acc: 0.813\n",
            "Epoch: 072 Train Loss: 0.373 Train Acc: 0.870 Eval Loss: 0.598 Eval Acc: 0.811\n",
            "Epoch: 073 Train Loss: 0.367 Train Acc: 0.872 Eval Loss: 0.597 Eval Acc: 0.809\n",
            "Epoch: 074 Train Loss: 0.364 Train Acc: 0.872 Eval Loss: 0.574 Eval Acc: 0.817\n",
            "Epoch: 075 Train Loss: 0.366 Train Acc: 0.871 Eval Loss: 0.540 Eval Acc: 0.832\n",
            "Epoch: 076 Train Loss: 0.362 Train Acc: 0.873 Eval Loss: 0.539 Eval Acc: 0.828\n",
            "Epoch: 077 Train Loss: 0.359 Train Acc: 0.874 Eval Loss: 0.620 Eval Acc: 0.806\n",
            "Epoch: 078 Train Loss: 0.363 Train Acc: 0.874 Eval Loss: 0.551 Eval Acc: 0.822\n",
            "Epoch: 079 Train Loss: 0.358 Train Acc: 0.873 Eval Loss: 0.549 Eval Acc: 0.825\n",
            "Epoch: 080 Train Loss: 0.359 Train Acc: 0.874 Eval Loss: 0.556 Eval Acc: 0.821\n",
            "Epoch: 081 Train Loss: 0.358 Train Acc: 0.874 Eval Loss: 0.560 Eval Acc: 0.816\n",
            "Epoch: 082 Train Loss: 0.353 Train Acc: 0.875 Eval Loss: 0.561 Eval Acc: 0.817\n",
            "Epoch: 083 Train Loss: 0.356 Train Acc: 0.876 Eval Loss: 0.545 Eval Acc: 0.827\n",
            "Epoch: 084 Train Loss: 0.353 Train Acc: 0.876 Eval Loss: 0.603 Eval Acc: 0.812\n",
            "Epoch: 085 Train Loss: 0.352 Train Acc: 0.877 Eval Loss: 0.591 Eval Acc: 0.813\n",
            "Epoch: 086 Train Loss: 0.354 Train Acc: 0.877 Eval Loss: 0.598 Eval Acc: 0.809\n",
            "Epoch: 087 Train Loss: 0.346 Train Acc: 0.878 Eval Loss: 0.541 Eval Acc: 0.825\n",
            "Epoch: 088 Train Loss: 0.346 Train Acc: 0.879 Eval Loss: 0.575 Eval Acc: 0.815\n",
            "Epoch: 089 Train Loss: 0.343 Train Acc: 0.881 Eval Loss: 0.564 Eval Acc: 0.821\n",
            "Epoch: 090 Train Loss: 0.345 Train Acc: 0.879 Eval Loss: 0.522 Eval Acc: 0.838\n",
            "Epoch: 091 Train Loss: 0.344 Train Acc: 0.879 Eval Loss: 0.565 Eval Acc: 0.819\n",
            "Epoch: 092 Train Loss: 0.345 Train Acc: 0.880 Eval Loss: 0.645 Eval Acc: 0.802\n",
            "Epoch: 093 Train Loss: 0.343 Train Acc: 0.881 Eval Loss: 0.565 Eval Acc: 0.823\n",
            "Epoch: 094 Train Loss: 0.340 Train Acc: 0.879 Eval Loss: 0.540 Eval Acc: 0.829\n",
            "Epoch: 095 Train Loss: 0.339 Train Acc: 0.879 Eval Loss: 0.540 Eval Acc: 0.823\n",
            "Epoch: 096 Train Loss: 0.339 Train Acc: 0.881 Eval Loss: 0.587 Eval Acc: 0.817\n",
            "Epoch: 097 Train Loss: 0.340 Train Acc: 0.880 Eval Loss: 0.582 Eval Acc: 0.822\n",
            "Epoch: 098 Train Loss: 0.337 Train Acc: 0.882 Eval Loss: 0.549 Eval Acc: 0.826\n",
            "Epoch: 099 Train Loss: 0.336 Train Acc: 0.883 Eval Loss: 0.535 Eval Acc: 0.832\n",
            "Epoch: 100 Train Loss: 0.226 Train Acc: 0.921 Eval Loss: 0.450 Eval Acc: 0.864\n",
            "Epoch: 101 Train Loss: 0.181 Train Acc: 0.936 Eval Loss: 0.455 Eval Acc: 0.864\n",
            "Epoch: 102 Train Loss: 0.166 Train Acc: 0.942 Eval Loss: 0.457 Eval Acc: 0.865\n",
            "Epoch: 103 Train Loss: 0.153 Train Acc: 0.946 Eval Loss: 0.453 Eval Acc: 0.865\n",
            "Epoch: 104 Train Loss: 0.145 Train Acc: 0.949 Eval Loss: 0.460 Eval Acc: 0.868\n",
            "Epoch: 105 Train Loss: 0.139 Train Acc: 0.952 Eval Loss: 0.467 Eval Acc: 0.867\n",
            "Epoch: 106 Train Loss: 0.128 Train Acc: 0.955 Eval Loss: 0.469 Eval Acc: 0.870\n",
            "Epoch: 107 Train Loss: 0.124 Train Acc: 0.956 Eval Loss: 0.482 Eval Acc: 0.865\n",
            "Epoch: 108 Train Loss: 0.120 Train Acc: 0.958 Eval Loss: 0.482 Eval Acc: 0.869\n",
            "Epoch: 109 Train Loss: 0.115 Train Acc: 0.959 Eval Loss: 0.478 Eval Acc: 0.869\n",
            "Epoch: 110 Train Loss: 0.110 Train Acc: 0.962 Eval Loss: 0.488 Eval Acc: 0.870\n",
            "Epoch: 111 Train Loss: 0.107 Train Acc: 0.962 Eval Loss: 0.491 Eval Acc: 0.869\n",
            "Epoch: 112 Train Loss: 0.107 Train Acc: 0.962 Eval Loss: 0.495 Eval Acc: 0.872\n",
            "Epoch: 113 Train Loss: 0.099 Train Acc: 0.965 Eval Loss: 0.512 Eval Acc: 0.868\n",
            "Epoch: 114 Train Loss: 0.097 Train Acc: 0.966 Eval Loss: 0.515 Eval Acc: 0.867\n",
            "Epoch: 115 Train Loss: 0.095 Train Acc: 0.967 Eval Loss: 0.505 Eval Acc: 0.868\n",
            "Epoch: 116 Train Loss: 0.095 Train Acc: 0.966 Eval Loss: 0.524 Eval Acc: 0.864\n",
            "Epoch: 117 Train Loss: 0.090 Train Acc: 0.968 Eval Loss: 0.522 Eval Acc: 0.866\n",
            "Epoch: 118 Train Loss: 0.086 Train Acc: 0.969 Eval Loss: 0.528 Eval Acc: 0.869\n",
            "Epoch: 119 Train Loss: 0.087 Train Acc: 0.970 Eval Loss: 0.529 Eval Acc: 0.869\n",
            "Epoch: 120 Train Loss: 0.085 Train Acc: 0.970 Eval Loss: 0.521 Eval Acc: 0.870\n",
            "Epoch: 121 Train Loss: 0.082 Train Acc: 0.971 Eval Loss: 0.536 Eval Acc: 0.867\n",
            "Epoch: 122 Train Loss: 0.079 Train Acc: 0.973 Eval Loss: 0.538 Eval Acc: 0.867\n",
            "Epoch: 123 Train Loss: 0.080 Train Acc: 0.973 Eval Loss: 0.555 Eval Acc: 0.865\n",
            "Epoch: 124 Train Loss: 0.073 Train Acc: 0.974 Eval Loss: 0.551 Eval Acc: 0.867\n",
            "Epoch: 125 Train Loss: 0.076 Train Acc: 0.973 Eval Loss: 0.548 Eval Acc: 0.867\n",
            "Epoch: 126 Train Loss: 0.073 Train Acc: 0.974 Eval Loss: 0.545 Eval Acc: 0.868\n",
            "Epoch: 127 Train Loss: 0.072 Train Acc: 0.975 Eval Loss: 0.553 Eval Acc: 0.869\n",
            "Epoch: 128 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.555 Eval Acc: 0.870\n",
            "Epoch: 129 Train Loss: 0.067 Train Acc: 0.976 Eval Loss: 0.555 Eval Acc: 0.869\n",
            "Epoch: 130 Train Loss: 0.067 Train Acc: 0.976 Eval Loss: 0.564 Eval Acc: 0.868\n",
            "Epoch: 131 Train Loss: 0.068 Train Acc: 0.976 Eval Loss: 0.568 Eval Acc: 0.865\n",
            "Epoch: 132 Train Loss: 0.065 Train Acc: 0.977 Eval Loss: 0.569 Eval Acc: 0.867\n",
            "Epoch: 133 Train Loss: 0.064 Train Acc: 0.977 Eval Loss: 0.576 Eval Acc: 0.867\n",
            "Epoch: 134 Train Loss: 0.063 Train Acc: 0.978 Eval Loss: 0.579 Eval Acc: 0.868\n",
            "Epoch: 135 Train Loss: 0.061 Train Acc: 0.979 Eval Loss: 0.576 Eval Acc: 0.868\n",
            "Epoch: 136 Train Loss: 0.061 Train Acc: 0.979 Eval Loss: 0.589 Eval Acc: 0.868\n",
            "Epoch: 137 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.598 Eval Acc: 0.869\n",
            "Epoch: 138 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.599 Eval Acc: 0.865\n",
            "Epoch: 139 Train Loss: 0.059 Train Acc: 0.980 Eval Loss: 0.599 Eval Acc: 0.867\n",
            "Epoch: 140 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.597 Eval Acc: 0.869\n",
            "Epoch: 141 Train Loss: 0.056 Train Acc: 0.980 Eval Loss: 0.601 Eval Acc: 0.866\n",
            "Epoch: 142 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.598 Eval Acc: 0.868\n",
            "Epoch: 143 Train Loss: 0.053 Train Acc: 0.981 Eval Loss: 0.600 Eval Acc: 0.866\n",
            "Epoch: 144 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.597 Eval Acc: 0.869\n",
            "Epoch: 145 Train Loss: 0.054 Train Acc: 0.981 Eval Loss: 0.595 Eval Acc: 0.866\n",
            "Epoch: 146 Train Loss: 0.054 Train Acc: 0.980 Eval Loss: 0.608 Eval Acc: 0.867\n",
            "Epoch: 147 Train Loss: 0.052 Train Acc: 0.981 Eval Loss: 0.608 Eval Acc: 0.866\n",
            "Epoch: 148 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.607 Eval Acc: 0.868\n",
            "Epoch: 149 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.627 Eval Acc: 0.865\n",
            "Epoch: 150 Train Loss: 0.044 Train Acc: 0.984 Eval Loss: 0.606 Eval Acc: 0.868\n",
            "Epoch: 151 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.603 Eval Acc: 0.869\n",
            "Epoch: 152 Train Loss: 0.036 Train Acc: 0.988 Eval Loss: 0.607 Eval Acc: 0.869\n",
            "Epoch: 153 Train Loss: 0.037 Train Acc: 0.988 Eval Loss: 0.598 Eval Acc: 0.870\n",
            "Epoch: 154 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.602 Eval Acc: 0.871\n",
            "Epoch: 155 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.599 Eval Acc: 0.873\n",
            "Epoch: 156 Train Loss: 0.034 Train Acc: 0.989 Eval Loss: 0.603 Eval Acc: 0.869\n",
            "Epoch: 157 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.600 Eval Acc: 0.872\n",
            "Epoch: 158 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.601 Eval Acc: 0.871\n",
            "Epoch: 159 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.604 Eval Acc: 0.870\n",
            "Epoch: 160 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.605 Eval Acc: 0.872\n",
            "Epoch: 161 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.607 Eval Acc: 0.870\n",
            "Epoch: 162 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.606 Eval Acc: 0.871\n",
            "Epoch: 163 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.608 Eval Acc: 0.871\n",
            "Epoch: 164 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.611 Eval Acc: 0.872\n",
            "Epoch: 165 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.611 Eval Acc: 0.870\n",
            "Epoch: 166 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.613 Eval Acc: 0.873\n",
            "Epoch: 167 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.619 Eval Acc: 0.871\n",
            "Epoch: 168 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.612 Eval Acc: 0.872\n",
            "Epoch: 169 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.616 Eval Acc: 0.871\n",
            "Epoch: 170 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.620 Eval Acc: 0.872\n",
            "Epoch: 171 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.621 Eval Acc: 0.871\n",
            "Epoch: 172 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.619 Eval Acc: 0.870\n",
            "Epoch: 173 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.622 Eval Acc: 0.872\n",
            "Epoch: 174 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.624 Eval Acc: 0.871\n",
            "Epoch: 175 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.627 Eval Acc: 0.871\n",
            "Epoch: 176 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.628 Eval Acc: 0.872\n",
            "Epoch: 177 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.628 Eval Acc: 0.871\n",
            "Epoch: 178 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.633 Eval Acc: 0.870\n",
            "Epoch: 179 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.633 Eval Acc: 0.870\n",
            "Epoch: 180 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.632 Eval Acc: 0.870\n",
            "Epoch: 181 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.638 Eval Acc: 0.870\n",
            "Epoch: 182 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.633 Eval Acc: 0.871\n",
            "Epoch: 183 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.640 Eval Acc: 0.870\n",
            "Epoch: 184 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.639 Eval Acc: 0.870\n",
            "Epoch: 185 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.644 Eval Acc: 0.871\n",
            "Epoch: 186 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.636 Eval Acc: 0.872\n",
            "Epoch: 187 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.643 Eval Acc: 0.872\n",
            "Epoch: 188 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.640 Eval Acc: 0.872\n",
            "Epoch: 189 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.644 Eval Acc: 0.870\n",
            "Epoch: 190 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.646 Eval Acc: 0.871\n",
            "Epoch: 191 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.640 Eval Acc: 0.871\n",
            "Epoch: 192 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.642 Eval Acc: 0.870\n",
            "Epoch: 193 Train Loss: 0.022 Train Acc: 0.992 Eval Loss: 0.644 Eval Acc: 0.871\n",
            "Epoch: 194 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.645 Eval Acc: 0.873\n",
            "Epoch: 195 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.652 Eval Acc: 0.872\n",
            "Epoch: 196 Train Loss: 0.021 Train Acc: 0.994 Eval Loss: 0.649 Eval Acc: 0.872\n",
            "Epoch: 197 Train Loss: 0.021 Train Acc: 0.994 Eval Loss: 0.648 Eval Acc: 0.872\n",
            "Epoch: 198 Train Loss: 0.020 Train Acc: 0.994 Eval Loss: 0.658 Eval Acc: 0.870\n",
            "Epoch: 199 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.658 Eval Acc: 0.871\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
            "Training QAT Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:179: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: -1 Eval Loss: 0.658 Eval Acc: 0.871\n",
            "Epoch: 000 Train Loss: 0.045 Train Acc: 0.984 Eval Loss: 0.746 Eval Acc: 0.854\n",
            "Epoch: 001 Train Loss: 0.047 Train Acc: 0.983 Eval Loss: 0.682 Eval Acc: 0.864\n",
            "Epoch: 002 Train Loss: 0.044 Train Acc: 0.984 Eval Loss: 0.703 Eval Acc: 0.859\n",
            "Epoch: 003 Train Loss: 0.042 Train Acc: 0.986 Eval Loss: 0.703 Eval Acc: 0.861\n",
            "Epoch: 004 Train Loss: 0.042 Train Acc: 0.985 Eval Loss: 0.683 Eval Acc: 0.866\n",
            "Epoch: 005 Train Loss: 0.045 Train Acc: 0.984 Eval Loss: 0.719 Eval Acc: 0.859\n",
            "Epoch: 006 Train Loss: 0.039 Train Acc: 0.987 Eval Loss: 0.715 Eval Acc: 0.858\n",
            "Epoch: 007 Train Loss: 0.041 Train Acc: 0.985 Eval Loss: 0.706 Eval Acc: 0.865\n",
            "Epoch: 008 Train Loss: 0.038 Train Acc: 0.987 Eval Loss: 0.708 Eval Acc: 0.863\n",
            "Epoch: 009 Train Loss: 0.040 Train Acc: 0.986 Eval Loss: 0.723 Eval Acc: 0.858\n",
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.07727417349815369, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.02895454689860344, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08111564069986343, zero_point=70, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.09911078959703445, zero_point=48\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.026185529306530952, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07247109711170197, zero_point=73, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.10325907170772552, zero_point=55\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.030393417924642563, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.06422974169254303, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.05344396084547043, zero_point=62)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.07767345011234283, zero_point=62\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.020634451881051064, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.057907965034246445, zero_point=75, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.08093485236167908, zero_point=57\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.020863762125372887, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04286956414580345, zero_point=74, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.02837895043194294, zero_point=64)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.04600120335817337, zero_point=68\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0046912627294659615, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.02665487863123417, zero_point=78, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.035937175154685974, zero_point=56\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.012101302854716778, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.02823738381266594, zero_point=62, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.03977849334478378, zero_point=53)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.044975195080041885, zero_point=51\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.002113250782713294, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.014240671880543232, zero_point=61, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.0384356752038002, zero_point=25\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.2807304263114929, zero_point=48, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n",
            "FP32 evaluation accuracy: 0.871\n",
            "INT8 evaluation accuracy: 0.858\n",
            "FP32 CPU Inference Latency: 7.09 ms / sample\n",
            "FP32 CUDA Inference Latency: 4.71 ms / sample\n",
            "INT8 CPU Inference Latency: 4.40 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 1.27 ms / sample\n"
          ]
        }
      ]
    }
  ]
}